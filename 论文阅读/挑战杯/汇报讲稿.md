- 尊敬的各位老师、各位评委、亲爱的同学们
	- 大家好！今天非常荣幸能够在这里向大家介绍我们的正在推进中的项目——基于大模型的任务理解与规划系统及结合仿真场景的具身验证。
- 首先请允许我来阐释我们想要实现的任务以及历史上的解决方案和它们的不足：
	- 在日常生活中，我们经常会收到来自于父母，家长的诸如“整理茶几”、“打扫厨房”或“收拾书桌”之类的指令。这些看似简单的任务，实际上涉及到一个复杂的认知与执行过程：智能体需要基于自然语言指令，理解任务目标，并与环境中的物体进行交互以完成任务。对于人类而言，这种任务执行过程是直观且自然的，但对于机器人来说，却是一个极具挑战性的问题。这一挑战的核心在于，机器人不仅需要准确理解自然语言指令，还需要在复杂的物理环境中进行任务规划与运动控制（即TAMP问题），以实现高效的任务执行。
- 让我们来看看过去上对这一问题的解决方案与其局限性
	- 首先是在深度学习方法还没有流行的时代，使用传统的控制方法的实现方案 Strip（1971） 与 PDDL（2003），尽管有效且高速，但这种方法需要专家级知识才能将问题抽象为正式的表示形式，从而限制了普通用户的可访问性。同时能够解决的任务范围受限制于预先定义的任务中，没有泛化能力。
	- 然后是深度学习方法开始流行时StructDiffusion，StructFormer 等基于深度学习方法的工作，这些方法的一个共同缺点是它们完全依赖于离线训练阶段，这使得它们仅适用于经过训练的对象类别和空间模式。而对于训练时没有的类似场景，它们不能做到很好的泛化。
	- 然后是基于大语言模型方法的 LLM-GROP（Grounded RObot Task and Motion Planning），它利用了大语言模型方法强大的生成能力，但其缺憾是由于在设计之初，它只能处理成对关系，因此无法执行三个物体及以上的更加复杂的任务，如物体重排任务
- 针对这一任务，我们提供了我们设计的实现方案。
	- 首先我们用大模型来对输入的 Instruction 进行处理，获取其中待操作物体的集合
	- 然后我们用开源的 Grounded SAM 模型根据集合对 RGB 深度四通道的image 进行分割，得到待操作物体掩码
	- 在模块2到模块3的部分，我们希望利用 RGD-B 图像重建的点云，在大模型的处理后得到平移自由度的可由传统任务规划器识别的目标
	- 在模块2到模块4的部分，我们利用在仿真环境的强化学习方法，再结合大模型的处理，得到旋转自由度的可由传统任务规划器识别的目标
	- 最后我们将这六个自由度的目标交由传统规划器求解，得到最终的目标轨迹执行方案，并由现实中的机械臂去具体施行
- 最后是我们的方案亮点。
- 相比于历史解决方案，由于我们在架构的优势，我们的模型的泛化能力要更强能够完成由LLM理解的更广泛的任务
- 同时，借助于大模型的强大处理能力，我们可以处理更为复杂的长序列任务
- 最终，我们的愿景是希望实现从真实世界到语言推理的桥梁
- 当然，我们的架构也有一些缺陷。由于无法由大语言模型直接输出控制层面的指令，我们不能实现端到端的效果
