---
title: easy-rl-chapter2
tags: 
categories: dairy
date: " 2025-02-12T19:13:52+08:00 "
modify: " 2025-02-12T19:13:52+08:00 "
dir: dairy
share: false
cdate: " 2025-02-12 "
mdate: " 2025-02-12 "
---

# 第2章 马尔可夫决策过程（MDP）总结

## 2.1 马尔可夫过程（MP）

### 2.1.1 马尔可夫性质

- **定义**：未来状态仅依赖当前状态，与历史无关：

$$
  p(X_{t+1} | X_{0:t}) = p(X_{t+1} | X_t)
$$

- **意义**：简化状态转移建模，无需考虑完整历史。

### 2.1.2 马尔可夫链

- **状态转移矩阵**：描述状态间转移概率的矩阵：

$$
  
  P = \begin{pmatrix}
  p(s_1|s_1) & \cdots & p(s_N|s_1) \\
  \vdots & \ddots & \vdots \\
  p(s_1|s_N) & \cdots & p(s_N|s_N)
  \end{pmatrix}
$$

- **示例**：图2.2中的4状态马尔可夫链，转移概率如  
![image.png]()https://raw.githubusercontent.com/Tendourisu/images/master/202502121927794.png)

---

## 2.2 马尔可夫奖励过程（MRP）

### 2.2.1 回报与价值函数

- **回报**：折扣奖励之和：

$$
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
$$

- **价值函数**：期望回报：

$$
  V(s) = \mathbb{E}[G_t | s_t = s]
$$

>[!tip]+ 使用折扣因子 $\lambda$ 的原因
>- 有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励
>- 我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励
>- 如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。最后，我们也更想得到即时奖励
>- 有些时候可以把折扣因子设为 0（γ=0γ=0），我们就只关注当前的奖励。我们也可以把折扣因子设为 1（γ=1γ=1），对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数（hyperparameter）来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体

### 2.2.2 贝尔曼方程

- **公式**：状态价值分解为即时奖励与未来折扣价值：

$$
V(s) = R(s) + \gamma \sum_{s'} p(s'|s) V(s')
$$

- **矩阵形式**：

$$
\begin{align}
V &= R + \gamma P V \quad \Rightarrow \quad \\ V &= (I - \gamma P)^{-1} R
\end{align}
$$

- **解法**：
	- 蒙特卡洛采样：  
		从某个状态开始，把小船放到状态转移矩阵里面，让它“随波逐流”，这样就会产生一个轨迹。产生一个轨迹之后，就会得到一个奖励，那么直接把折扣的奖励即回报 $g$ 算出来。算出来之后将它积累起来，得到回报 $G_{t}$ ​。当积累了一定数量的轨迹之后，我们直接用 $G_{t}$ ​ 除以轨迹数量，就会得到某个状态的价值。  
	 ![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502130000902.png)

	- 时序差分学习。

---

## 2.3 马尔可夫决策过程（MDP）

### 2.3.1 核心概念

- **动作空间**：状态转移和奖励函数依赖动作：

$$
  p(s'|s,a), \quad R(s,a)
$$

- **策略**：动作选择概率分布：

$$
  \pi(a|s) = p(a_t=a | s_t=s)
$$

### 2.3.2 价值函数与Q函数

- **状态价值函数**：

$$
  V_\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]
$$

- **动作价值函数（Q函数）**：

$$
  Q_\pi(s,a) = \mathbb{E}_\pi[G_t | s_t = s, a_t = a]
$$

- **关系**：

$$
  V_\pi(s) = \sum_a \pi(a|s) Q_\pi(s,a)
$$

### 2.3.3 贝尔曼期望方程

- **状态价值方程**：

$$
  V_\pi(s) = \sum_a \pi(a|s) \left( R(s,a) + \gamma \sum_{s'} p(s'|s,a) V_\pi(s') \right)
$$

- **Q函数方程**：

$$
  Q_\pi(s,a) = R(s,a) + \gamma \sum_{s'} p(s'|s,a) V_\pi(s'
$$

---

## 2.4 策略评估与控制

### 2.4.1 策略评估（预测）

- **目标**：计算给定策略的价值函数。
- **方法**：迭代应用贝尔曼期望方程：

$$
  V_{k+1}(s) = \sum_a \pi(a|s) \left( R(s,a) + \gamma \sum_{s'} p(s'|s,a) V_k(s') \right)
$$

### 2.4.2 策略迭代

- **步骤**：
  1. **策略评估**：计算当前策略的价值函数。
  2. **策略改进**：贪心更新策略：

$$
     \pi_{\text{new}}(s) = \arg\max_a Q_\pi(s,a)
$$

- **特点**：保证收敛到最优策略。

### 2.4.3 价值迭代

- **贝尔曼最优方程**：

$$
  V^*(s) = \max_a \left( R(s,a) + \gamma \sum_{s'} p(s'|s,a) V^*(s') \right)
$$

- **步骤**：直接迭代更新价值函数至收敛，再提取最优策略。

### 2.4.4 对比

| **方法**       | **核心方程**       | **步骤**                     | **应用场景**       |
|----------------|--------------------|------------------------------|--------------------|
| 策略迭代       | 贝尔曼期望方程     | 评估→改进→重复               | 精确策略优化       |
| 价值迭代       | 贝尔曼最优方程     | 直接迭代价值函数→提取策略    | 快速收敛到最优价值 |

---

## 关键图表说明

1. **图2.1 智能体与环境交互**：  
   - 智能体根据状态选择动作，环境返回新状态和奖励，形成闭环。
2. **图2.5 状态转移**：  
   - 状态`s1`转移到`s2`、`s4`的概率分别为0.2和0.7，体现马尔可夫性质。
3. **图2.10 备份图**：  
   - 空心节点表示状态，实心节点表示动作，箭头表示价值传递路径。

---

## 代码示例

### 动态规划策略评估伪代码

```python
def policy_evaluation(P, R, pi, gamma, theta=1e-6):
    V = np.zeros(len(S))
    while True:
        delta = 0
        for s in S:
            v = V[s]
            V[s] = sum(pi(a|s) * (R(s,a) + gamma * sum(P(s'|s,a)*V[s'] for s' in S))
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    return V
```

**逻辑**：通过贝尔曼期望方程迭代更新价值函数直至收敛。

---

## 核心概念总结

4. **马尔可夫性质**：未来仅依赖当前状态，简化建模。
5. **贝尔曼方程**：价值函数的递归分解，支撑动态规划。
6. **策略迭代 vs 价值迭代**：前者交替优化策略与价值，后者直接优化价值函数。
7. **探索与利用**：隐含在策略更新中（如ε-贪心策略未显式提及但相关）。

```
