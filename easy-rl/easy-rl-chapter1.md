---
title: easy-rl-chapter1
tags:
  - RL
categories: dairy
date: " 2025-02-11T12:48:32+08:00 "
modify: " 2025-02-11T12:48:32+08:00 "
dir: dairy
share: false
cdate: " 2025-02-11 "
mdate: " 2025-02-11 "
---
# 第1章 强化学习基础 总结

## 1.1 强化学习概述
### 核心定义
- **强化学习（RL）**：智能体通过与环境交互最大化累积奖励的学习范式。
- **基本组成**：智能体（Agent）与环境（Environment）持续交互，输出动作（Action）并接收状态（State）和奖励（Reward）。
![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502111525320.png)

### 强化学习 vs 监督学习
| **对比维度**       | **监督学习**                   | **强化学习**                     |
|--------------------|-------------------------------|---------------------------------|
| 数据特性           | 独立同分布（i.i.d.）          | 时间序列数据（关联性强）         |
| 反馈方式           | 即时标签反馈                  | 延迟奖励信号                     |
| 目标               | 拟合标注数据                  | 最大化长期奖励                   |
| 探索-利用          | 无                            | 需平衡探索新动作与利用已知动作   |
### 标准强化学习 vs 深度强化学习
- 标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程，如下图所示。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。
- 深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个端到端训练（end-to-end training）的过程，如下图所示。我们不需要设计特征，直接输入状态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程。
![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502111556866.png)

![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502111556845.png)

### 关键概念
- **延迟奖励**：动作的长期影响需通过多步交互体现（如雅达利游戏最终得分）。
- **探索与利用**：探索尝试新动作可能获得更高奖励，利用执行已知最优动作。
- **马尔可夫决策过程（MDP）**：完全可观测环境下的数学建模框架。
- **部分可观测马尔可夫决策过程（POMDP）**：智能体仅能获取部分环境状态信息。

### 应用实例
- AlphaGo：超越人类棋手的深度强化学习模型。
- [机械臂控制Deep Learning for Robots: Learning from Large-Scale Interaction](https://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html)：通过虚拟环境训练后迁移到真实机器人。
- 游戏AI：如雅达利Pong游戏，智能体通过试错学习策略。

---

## 1.2 序列决策
### 核心流程
1. **交互循环**：智能体根据当前状态选择动作，环境返回新状态和奖励。
   - 公式：$H_t = o_1, a_1, r_1, \ldots, o_t, a_t, r_t$
2. **状态与观测**：
   - **状态（State） $S_t$** ：环境的完整描述。整个游戏的状态看成关于历史的函数 $$S_{t}=f(H_{t})$$
   - **观测（Observation） $O_t$** ：状态的部分信息（可能不完整）。
   - **环境状态**有自己的函数 $s_{t}^e=f^e(H_{t)})$ 来更新状态
   - **agent状态**有自己的函数 $s_{t}^a=f^a(H_{t)})$ 来更新状态
   - 当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中， $$O_{t}=s_{t}^e=s_{t}^a$$
   - 当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。在这种情况下，强化学习通常被建模成**部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）** 的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔可夫决策过程可以用一个七元组描述： $$(S,A,T,R,Ω,O,γ)$$ 其中:
   -  $S$ 表示状态空间，为隐变量， $A$ 为动作空间， $T(s′∣s,a)$ 为状态转移概率， $R$ 为奖励函数， $Ω(o∣s,a)$ 为观测概率， $O$ 为观测空间， $γ$ 为折扣系数。
### 奖励设计
- **奖励信号**：标量值反馈，衡量动作的短期效果。
- **长期目标**：最大化期望累积奖励 $G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}$，其中 $\gamma$ 为折扣因子。

---

## 1.3 动作空间
- **离散动作空间**：有限个可选动作（如围棋的落子位置）。
- **连续动作空间**：动作值为实数向量（如机器人关节角度控制）。

---

## 1.4 强化学习智能体的组成
### 三大核心组件
1. **策略（Policy）**：
   - 随机策略：$\pi(a|s)$ 输出动作概率分布。
   - 确定策略：直接选择最优动作 $a^* = \arg\max_a \pi(a|s)$。
2. **价值函数（Value Function）**：
   - 状态价值函数：$V_\pi(s) = \mathbb{E}_\pi[G_t | s_t = s]$。
   - 动作价值函数（Q函数）：$Q_\pi(s,a) = \mathbb{E}_\pi[G_t | s_t = s, a_t = a]$。
3. **模型（Model）**：
   - 状态转移概率：$p(s'|s,a)$。
   - 奖励函数：$R(s,a) = \mathbb{E}[r_{t+1} | s_t = s, a_t = a]$。

### 智能体类型
| **类型**           | **特点**                               | **示例算法**         |
|--------------------|---------------------------------------|---------------------|
| 基于价值（Value-based） | 学习价值函数，隐式推导策略          | Q-Learning, DQN     |
| 基于策略（Policy-based） | 直接学习策略函数                 | REINFORCE, PPO      |
| 演员-评论员（Actor-Critic） | 同时学习策略和价值函数       | A2C, SAC           |
| 有模型（Model-based）   | 显式建模环境动态                  | Dyna-Q             |
| 免模型（Model-free）    | 不依赖环境模型，直接交互学习     | 大多数深度强化学习算法 |

---

## 1.5 探索与利用
### 经典问题：K-臂赌博机
- **目标**：通过有限次尝试最大化累积奖励。
- **策略**：
  - 仅探索：均匀尝试所有动作，精确估计奖励期望。
  - 仅利用：始终选择当前最优动作，可能错过更高奖励动作。
- **平衡方法**：ε-贪心、UCB（Upper Confidence Bound）等。

---

## 1.6 强化学习实验（Gym库）
### 环境搭建示例
```python
import gym
env = gym.make("CartPole-v0")  # 创建环境
observation = env.reset()       # 初始化状态

for _ in range(1000):
    action = env.action_space.sample()  # 随机采样动作
    observation, reward, done, info = env.step(action)  # 执行动作
    if done:
        observation = env.reset()  # 回合结束，重置环境
env.close()
```
**代码说明**：  
- `gym.make()` 创建经典控制问题CartPole环境。
- `env.step(action)` 返回四元组：新状态、即时奖励、终止标志、调试信息。

### 关键环境参数
| **环境**         | **观测空间**              | **动作空间**       | **目标**                     |
|------------------|--------------------------|-------------------|-----------------------------|
| CartPole-v0      | 4维向量（位置、速度等）   | 离散（左/右）     | 保持杆直立尽可能久           |
| MountainCar-v0   | 2维向量（位置、速度）     | 离散（左/右/不动）| 驱动小车到达山顶             |

---

## 1.7 核心图表说明
### 图1.1 强化学习示意
- **逻辑**：智能体接收状态 $s_t$，输出动作 $a_t$；环境返回新状态 $s_{t+1}$ 和奖励 $r_{t}$。

### 图1.10 标准强化学习 vs 深度强化学习
- **标准RL**：人工设计特征 + 价值函数（如TD-Gammon）。
- **深度RL**：端到端训练，神经网络直接映射状态到动作（如DQN）。

### 图1.15 马尔可夫决策过程
- **流程**：状态 $s$ → 动作 $a$ → 状态转移 $s'$ 和奖励 $r$，循环往复。
```mermaid
graph LR
    S(当前状态 s) --> A(动作 a)
    A --> T[状态转移 p(s'|s,a)]
    T --> R[奖励函数 r(s,a)]
    R --> S'(下一状态 s')
``` 

---

## 参考文献
- 关键教材：《强化学习：原理与Python实现》《神经网络与深度学习》
- 实践工具：OpenAI Gym、PyTorch、TensorFlow
- 扩展阅读：David Silver强化学习公开课笔记、DeepMind系列论文
```