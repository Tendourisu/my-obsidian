---
title: easy-rl-chapter3
tags:
  - RL
categories: dairy
date: " 2025-02-14T01:08:00+08:00 "
modify: " 2025-02-14T01:08:00+08:00 "
dir: dairy
share: false
cdate: " 2025-02-14 "
mdate: " 2025-02-14 "
---

# 第3章 表格型方法总结

## 3.1 马尔可夫决策过程（MDP）

### 核心概念

- **四元组**：$(S, A, P, R)$，分别表示状态集合、动作集合、状态转移概率、奖励函数。
- **马尔可夫性质**：下一时刻状态仅依赖当前状态和动作，不依赖历史。
- **有模型 vs 免模型**：
  - **有模型**：已知$P$和$R$，可用动态规划（策略迭代、价值迭代）。
  - **免模型**：未知$P$和$R$，依赖试错探索（如蒙特卡洛、时序差分）。

### 状态转移示例

$$
P(s_{t+1}, r_t | s_t, a_t)
$$

表示在状态$s_t$执行动作$a_t$后转移到$s_{t+1}$并获得奖励$r_t$的概率。

---

## 3.2 Q表格

### 核心概念

- **Q函数**：$Q(s, a)$表示在状态$s$选择动作$a$的长期期望回报。
- **Q表格**：行代表状态，列代表动作，存储每个状态-动作对的Q值。
- **折扣因子$\gamma$**：平衡即时奖励与未来奖励的重要性（$\gamma=0$仅看单步，$\gamma=1$考虑无限步）。

### 示例：悬崖行走问题

- 目标：从起点到终点，避免掉入悬崖。
- 奖励：每走一步$-1$，掉崖$-100$。
- Q值更新逻辑：

$$
  G_t = r_{t+1} + \gamma G_{t+1} \quad (\text{从后往前递推})
$$

![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502171831301.png)

---

## 3.3 免模型预测

### 方法对比

| **方法**       | **更新方式**                          | **特点**                               |
|----------------|--------------------------------------|----------------------------------------|
| **蒙特卡洛**   | 使用完整轨迹的回报均值更新Q值          | 高方差，需完整轨迹，无偏估计           |
| **时序差分**   | 单步更新（TD目标 = 即时奖励 + γ·下一状态Q值） | 低方差，可在线学习，有偏估计           |

### 蒙特卡洛(MC)更新公式

$$
V(s_t) \leftarrow V(s_t) + \alpha \left[ G_{t} - V(s_t) \right]
$$

- 其中， $\alpha$ 代表的是学习率，可以人为设置
- $G_t$ 是把一条轨迹跑完后才得出来的回报

### 时序差分（TD）更新公式

$$
V(s_t) \leftarrow V(s_t) + \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]
$$

- **TD误差**： $\delta = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$ ，其中 $\delta = r_{t+1} + \gamma V(s_{t+1})$ 被称为**时序差分目标（TD target）**，时序差分目标是带衰减的未来奖励的总和。
- **物理意义**：类似巴甫洛夫条件反射，通过相邻状态的价值迭代更新。  
![image.png](https://raw.githubusercontent.com/Tendourisu/images/master/202502171835268.png)

### n步时序差分（n-step TD）

$$
\begin{align}
n &= 1  (\text{TD})  &G_t^{(1)} = r_{t+1} + \gamma V(s_{t+1}) \\
n &= 2  &G_t^{(2)} = r_{t+1} + \gamma r_{t+2} + \gamma^2 V(s_{t+2}) \\
\vdots \\
n &= \infty \ (\text{MC}) &G_t^{\infty} = r_{t+1} + \gamma r_{t+2} + \cdots + \gamma^{T-t-1} r_T
\end{align}
$$

得到时序差分目标之后，我们用增量式学习（incremental learning）的方法来更新状态的价值：

$$
V(s_{t})←V(s_{t})+α(G_{t}^n−V(s_{t}))
$$

---

## 3.4 免模型控制

### Sarsa（同策略）

- **更新规则**：

  ```math
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
  ```

- **特点**：使用实际执行的动作$a_{t+1}$更新Q值，策略保守，适合高风险环境（如悬崖行走）。

### Q学习（异策略）

- **更新规则**：

  ```math
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
  ```

- **特点**：使用目标策略（贪心）更新Q值，行为策略（如ε-贪心）探索环境，策略激进，更快收敛。

### 同策略 vs 异策略

| **对比项**   | **同策略（Sarsa）**                  | **异策略（Q学习）**                |
|--------------|--------------------------------------|------------------------------------|
| **策略一致性** | 行为
